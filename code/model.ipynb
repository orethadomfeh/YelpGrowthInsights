{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model \n",
    "\n",
    "**Why Random Forest Model?** \n",
    "\n",
    "As a data scientist, my goal is to select a model that provides the best balance of accuracy, interpretability, and efficiency, while addressing the specific requirements of the project. Here are some of the key reasons I chose the Random Forest model for this task:\n",
    "\n",
    "Accuracy: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. By aggregating the results of multiple trees, Random Forest typically achieves higher accuracy compared to a single decision tree. The model has the ability to capture complex patterns and relationships in the data, making it well-suited for this task.\n",
    "\n",
    "Robustness to Overfitting: One of the main challenges when working with decision trees is their tendency to overfit the training data. Random Forest mitigates this issue by averaging the predictions of multiple trees, resulting in a more generalized model that is less susceptible to overfitting.\n",
    "\n",
    "Handling of Missing and Categorical Data: Random Forest can handle missing data and categorical features more effectively than some other models, making it a good choice for datasets that may have missing values or a mix of categorical and numerical features, like the Yelp dataset.\n",
    "\n",
    "Parallelizability: Random Forest can be easily parallelized, meaning that it can take advantage of multi-core processors to speed up the training process. This is particularly beneficial when working with large datasets, as it allows the model to be trained more quickly.\n",
    "\n",
    "Feature Importance: Random Forest provides an easy way to estimate the importance of each feature in making predictions. This can be valuable for understanding which variables are the most influential in the model and may help guide further feature engineering or model selection efforts.\n",
    "\n",
    "Model Flexibility: Random Forest can be applied to both classification and regression problems, making it a versatile choice that can be adapted to a wide range of tasks.\n",
    "\n",
    "Given these advantages, the Random Forest model was chosen as a suitable option for predicting the star rating of a business using the Yelp dataset. However, it's important to note that other models could be explored as well, depending on the specific needs of the project and the desired trade-offs between accuracy, interpretability, and computational efficiency.\n",
    "\n",
    "**Alternatives** \n",
    "\n",
    "There are several alternative models that could be considered for this task. I'll outline a few of them and explain why they were not chosen in the previous code snippet:\n",
    "\n",
    "Logistic Regression: Logistic Regression is a simple and interpretable model that works well for binary classification tasks. However, in this case, we have a multi-class classification problem, and while logistic regression can be extended to handle multi-class problems, it may not perform as well as other models when dealing with complex relationships between features. Moreover, logistic regression assumes a linear relationship between features and the log-odds of the target class, which may not hold true for this dataset.\n",
    "\n",
    "Support Vector Machines (SVM): SVM is a powerful classification model that works well with high-dimensional data and can capture complex decision boundaries. However, SVM can be computationally expensive, especially with large datasets, and may not scale well to the size of the Yelp dataset. Additionally, SVM is less interpretable than some other models, which may be a consideration for certain projects.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN is a simple and intuitive model that can work well for classification tasks. However, KNN can be sensitive to the choice of the number of neighbors (k) and the distance metric used. Additionally, KNN can be computationally expensive for large datasets, as it requires calculating the distance between each data point and all other data points in the dataset. This may be a concern when working with a large dataset like the Yelp data.\n",
    "\n",
    "Neural Networks: Neural networks, particularly deep learning models, can achieve high accuracy on complex classification tasks. However, they can be computationally expensive to train and require a large amount of data to perform well. Moreover, they are less interpretable than other models, making it difficult to understand the relationships between features and the target variable. In this case, the added complexity of a neural network may not be necessary, given that simpler models like Random Forest can achieve good performance while being more interpretable and computationally efficient.\n",
    "\n",
    "In the given context, the Random Forest model was chosen because it balances accuracy, interpretability, and computational efficiency while being robust to overfitting and able to handle missing and categorical data. However, depending on the specific requirements of a project, it might be worthwhile to explore alternative models, as each model has its own strengths and weaknesses. In practice, data scientists often try multiple models and compare their performance to choose the most suitable one for the task at hand.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "preprocessed_data = pd.read_csv('../data/preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and target variable\n",
    "features = preprocessed_data[['review_count', 'stars_review']]\n",
    "target = preprocessed_data['stars_business'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 1001, 50),\n",
    "    'max_depth': [None] + list(np.arange(10, 51, 5)),\n",
    "    'min_samples_split': np.arange(2, 21, 2),\n",
    "    'min_samples_leaf': np.arange(1, 21, 2),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_classifier, param_distributions=param_dist, n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=13, min_samples_split=20, n_estimators=650; total time= 8.6min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=13, min_samples_split=20, n_estimators=650; total time= 8.6min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=13, min_samples_split=20, n_estimators=650; total time= 8.6min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=13, min_samples_split=20, n_estimators=650; total time= 8.7min\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the best hyperparameters on the test set\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy on the test set:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
